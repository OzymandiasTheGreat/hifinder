#!/usr/bin/env python3

import os
import re
from argparse import ArgumentParser
from pathlib import Path, PurePath
from shutil import copyfileobj
from urllib.parse import urlparse, parse_qs
from imghdr import what
import gi
# gir1.2-glib-2.0
# gir1.2-webkit-3.0
gi.require_version('WebKit', '3.0')
from gi.repository import WebKit, GLib
import requests
from bs4 import BeautifulSoup
from timeit import timeit


class CLI(object):

	def __init__(self):

		self.mimetypes = {
			'webp': ('webp',),
			'tiff': ('tif', 'tiff'),
			'gif': ('gif',),
			'png': ('png',),
			'jpeg': ('jpg', 'jpeg'),
			'bmp': ('bmp',)}
		self.images = []
		self.template = {
			'local_path': None,
			'local_type': None,
			'uri': None,
			'remote_type': None,
			'save_path': None,
			'error': None}
		self.cwd = None
		parser = ArgumentParser(
			prog='hifinder',
			description=('Download original/Hi-Res versions of local images.\n'
				+ 'Saves to working directory, unles one of -sobd is given.'))

		mode = parser.add_mutually_exclusive_group()
		mode.add_argument(
			'-s', '--subdir',
			help=('save to subdirectory in the source directory keeping remote'
				+ ' file names'),
			action='store_true')
		mode.add_argument(
			'-o', '--overwrite',
			help='overwrite files preserving local file names',
			action='store_true')
		mode.add_argument(
			'-b', '--by-side',
			help=('save remote files to the source directory keeping remote'
				+ ' file names'),
			action='store_true')
		mode.add_argument(
			'-d', '--discard',
			help=('save to source directory discarding (deleting) local files'
				+ ' and keep remote file names'),
			action='store_true')
		parser.add_argument(
			'-n', '--dir-name',
			help='use custom name in --subdir mode (defaults to Hi-Res)',
			default='Hi-Res')
		parser.add_argument(
			'-p', '--max-pages',
			help='maximum number of Google image search result pages to parse',
			type=int,
			default=5)
		parser.add_argument(
			'-q', '--quiet',
			help='surpress output and ignore errors',
			action='store_true')
		parser.add_argument(
			'image_path',
			help='one or more path to image file',
			nargs='+')

		args = parser.parse_args()
		self.check(args)
		self.find(args.max_pages)
		self.download(args)
		if not args.quiet:
			for image in self.images:
				if image['error']:
					print(image['local_path'].name, ':', image['error'])

	def check(self, args):

		if not any((args.subdir, args.overwrite, args.by_side, args.discard)):
			self.cwd = Path.cwd()
			if not os.access(str(self.cwd), os.W_OK):
				print('Permission denied: cannot save in current directory')

		for path in args.image_path:
			image = self.template.copy()
			image['local_path'] = Path(path).resolve()
			image['local_type'] = what(str(image['local_path']))
			if not image['local_path'].exists():
				image['error'] = 'File does not exist'
			elif not image['local_type'] in self.mimetypes:
				image['error'] = 'Unsupported file type'
			elif any((args.subdir, args.overwrite, args.by_side, args.discard)):
				if not os.access(str(image['local_path'].parent), os.W_OK):
					image['error'] = (
						'Permission denied: cannot write in source directory')
			self.images.append(image)

	def find(self, max_pages):

		renderer = Renderer()
		for image in self.images:
			if not image['error']:
				matches = Finder(image['local_path'], renderer, max_pages)
				if not matches.error:
					if matches.width >= matches.height:
						key_func = lambda match: match['width']
					else:
						key_func = lambda match: match['height']
					final = max(matches.matches, key=key_func)
					if key_func(matches) < key_func(final):
						image['uri'] = final['uri']
					else:
						image['error'] = 'No larger version available'
				else:
					image['error'] = matches.error
		del renderer

	def download(self, args):

		for image in self.images:
			if not image['error']:
				if args.subdir:
					image['save_path'] = (
						Path(image['local_path'].parent)
						/ args.dir_name
						/ PurePath(urlparse(image['uri']).path).name)
					image['save_path'].parent.mkdir(exist_ok=True)
				elif args.overwrite:
					image['save_path'] = image['local_path']
					image['local_path'].unlink()
				elif args.by_side:
					image['save_path'] = (
						Path(image['local_path'].parent)
						/ PurePath(urlparse(image['uri']).path).name)
				elif args.discard:
					image['save_path'] = (
						Path(image['local_path'].parent)
						/ PurePath(urlparse(image['uri']).path).name)
					image['local_path'].unlink()
				else:
					image['save_path'] = (
						Path.cwd()
						/ PurePath(urlparse(image['uri']).path).name)
				with requests.get(image['uri'], stream=True) as response:
					with image['save_path'].open('wb') as fd:
						copyfileobj(response.raw, fd)


class Finder(object):

	def __init__(self, path, renderer, max_pages):

		self.render = renderer.render
		self.quit = renderer.main_loop.quit
		self.max_pages = max_pages
		self.base = 'http://www.google.com'
		self.upload = 'http://www.google.com/searchbyimage/upload'
		self.match_size = re.compile(r'(?P<width>\d+)\s√ó\s(?P<height>\d+)')
		self.matches = []
		self.error = None
		self.search(path)

	def __getitem__(self, attr):

		return getattr(self, attr)

	def search(self, path):

		with path.open('rb') as fd:
			image = {'encoded_image': (str(path), fd), 'image_content': ''}
			response = requests.post(
				self.upload, files=image, allow_redirects=False)
		self.render(response.headers['Location'], self.parse, count=1)

	def parse(self, source, count):

		soup = BeautifulSoup(source, 'html.parser')

		try:
			if count == 1:
				size = self.match_size.match(soup.find(id='_w6').div.br.string)
				self.width = int(size.group('width'))
				self.height = int(size.group('height'))

				result_head = soup.find(class_='normal-header not-first')
				results = result_head.next_sibling
			else:
				results = soup
			proceed = True
		except AttributeError:
			self.error = str(soup.find('div', class_='card-section').string)
			self.quit()
			proceed = False
		if proceed:
			for entry in results.find_all(class_='g'):
				try:
					result = {}
					link = entry.find(class_='th _lyb').a['href']
					query = urlparse(link).query
					result['uri'] = parse_qs(query)['imgurl'][0]

					size = self.match_size.match(
						entry.find('span', class_='f').string)
					result['width'] = int(size.group('width'))
					result['height'] = int(size.group('height'))
					self.matches.append(result)
				except AttributeError:
					pass

			if count <= self.max_pages:
				try:
					next_page = soup.find(id='pnnext')['href']
					self.render(self.base + next_page, self.parse, count=count + 1)
				except TypeError:
					self.quit()
			else:
				self.quit()


class Renderer(object):

	def __init__(self):

		self.main_loop = GLib.MainLoop()
		self.renderer = WebKit.WebView()
		settings = WebKit.WebSettings()
		settings.props.auto_load_images = False
		self.renderer.set_settings(settings)
		self.renderer.connect('notify::load-status', self.handler)

	def handler(self, renderer, pspec):

		# For some reason WebKit is not visible from MainLoop
		if renderer.props.load_status == 2: # WebKit.LoadStatus.FINISHED:
			source = renderer.get_main_frame().get_data_source().get_data().str
			self.callback(source, *self.args, **self.kwargs)

	def render(self, url, callback, *args, **kwargs):

		self.callback = callback
		self.args = args
		self.kwargs = kwargs
		self.renderer.load_uri(url)
		if not self.main_loop.is_running():
			self.main_loop.run()

CLI()
